# ==============================================================================
# Kinetica configuration file.
# ==============================================================================

[gaia]

# ==============================================================================
# Version

# The current version of this configuration. This param is automatically updated
# by Kinetica and should not be edited manually.
file_version = 7.2.0.8.20240610164423

# ==============================================================================
# Identification

# Name of the ring containing the high availability clusters.
ring_name = default

# Name of the cluster containing all the nodes of the database.
cluster_name = test

# ==============================================================================
# Hosts
#
# Specify each unique host in the cluster. Each host runs a local instance
# of HostManager, which manages database services and components that are
# part of the Kinetica install.
#
# Host settings are defined as follows:
#
## host<#>.<parameter>
#
# Any number of hosts may be configured, starting with 'host0', and must
# be specified in consecutive order (e.g., 'host0', 'host1', 'host2').
#
# NOTE: For 'public_address' & 'host_manager_public_url', if either parameter is
#       to be specified, that parameter must be specified for all hosts.

# Host *parameters* include the following:
#
# * 'address'                : The unique address for this host. Single host
#                              clusters may use 127.0.0.1. This field is required.
# * 'public_address'         : An optional public address that clients should
#                              use when performing multi-head operations.
# * 'ha_address'             : An optional address to allow inter-cluster
#                              communication with HA when 'address' is not routable
#                              between clusters.
# * 'host_manager_public_url': An optional public URL that can be used to access
#                              HostManager endpoints. See 'host_manager_http_port'.
# * 'ram_limit'              : An optional upper limit for RAM (in bytes) on this
#                              host, capping the amount of memory that all Kinetica
#                              processes are allowed to consume.
#                              Use "-1" for no limit.
# * 'gpus'                   : An optional comma-separated list of GPUs (as
#                              reported by NVML) that may be reserved by Kinetica
#                              services (e.g., ranks, graph server, ML, etc.).
#                              Leave empty to make all GPUs available for use.
#                              The same failover caveat with the RAM limit also
#                              applies.
# * 'accepts_failover'       : Whether or not this host should accept failed 
#                              processes that need to be migrated off degraded
#                              hosts.  Default is to not accept failed processes.

host0.address = 192.168.250.15
host0.public_address = 192.168.250.15
host0.ha_address = 192.168.250.15
host0.host_manager_public_url = http://192.168.250.15:9300
host0.ram_limit = 218999949999
host0.gpus = 
host0.accepts_failover = false

host1.address = 192.168.250.16
host1.public_address = 192.168.250.16
host1.ha_address = 192.168.250.16
host1.host_manager_public_url = http://192.168.250.16:9300
host1.ram_limit = 219000800000
host1.gpus = 
host1.accepts_failover = false

host2.address = 192.168.250.17
host2.public_address = 192.168.250.17
host2.ha_address = 192.168.250.17
host2.host_manager_public_url = http://192.168.250.17:9300
host2.ram_limit = 219007600000
host2.gpus = 
host2.accepts_failover = false


# ==============================================================================
# Ranks

# Specify the host on which to run each rank worker process in the cluster.
# Multiple ranks may run on the same host. If a rank is specified with an empty
# host entry, it is effectively disabled (i.e., removed from the cluster) and
# no persisted data for this rank will be loaded even if present on any host.
# (e.g. 'rank2.host = host1' to run rank2 on host1)
#
rank0.host = host0
rank1.host = host0
rank2.host = host1
rank3.host = host2


# ==============================================================================
# Network

# Head HTTP server IP address.
# Set to the publicly accessible IP address of the first process, **rank0**.
head_ip_address = ${gaia.host0.address}

# Head HTTP server port to use for 'head_ip_address'.
head_port = 9191

# Set to "true" to use HTTPS; if "true" then 'https_key_file' and
# 'https_cert_file' must be provided
use_https = false

# Files containing the SSL private Key and the SSL certificate for.
# If required, a self signed certificate (expires after 10 years) can be
# generated via the command:
#
## openssl req -newkey rsa:2048 -new -nodes -x509 \
##   -days 3650 -keyout key.pem -out cert.pem
#
https_key_file = 
https_cert_file = 

# Value to return via Access-Control-Allow-Origin HTTP header
# (for Cross-Origin Resource Sharing).
# Set to empty to not return the header and disallow CORS.
http_allow_origin = *

# Keep HTTP connections alive between requests
http_keep_alive = false

# Start an HTTP server as a proxy to handle LDAP and/or Kerberos authentication.
# Each host will run an HTTP server and access to each rank is available through
# http://host:8082/gpudb-1, where port "8082" is defined by 'httpd_proxy_port'.
#
# NOTE: HTTP external endpoints are not affected by the 'use_https' parameter
#       above. If you wish to enable HTTPS, you must edit the
#       "/opt/gpudb/httpd/conf/httpd.conf" and setup HTTPS as per the Apache
#       httpd documentation at https://httpd.apache.org/docs/2.2/
enable_httpd_proxy = false

# TCP port that the httpd auth proxy server will listen on if
# 'enable_httpd_proxy' is "true".
httpd_proxy_port = 8082

# Set to "true" if the httpd auth proxy server is configured to use HTTPS.
httpd_proxy_use_https = false

# Trigger ZMQ publisher server port ("-1" to disable), uses the
# 'head_ip_address' interface.
trigger_port = 9001

# Set monitor ZMQ publisher server port ("-1" to disable), uses the 'head_ip_address' interface.
set_monitor_port = 9002

# Set monitor ZMQ publisher internal proxy server port ("-1" to disable), uses
# the 'head_ip_address' interface.
#
# IMPORTANT:  Disabling this port effectively prevents worker nodes from
# publishing set monitor notifications when multi-head ingest is enabled (see
# 'enable_worker_http_servers').
set_monitor_proxy_port = 9003

# Set monitor queue size
set_monitor_queue_size = 1000

# Enable Reveal runtime
enable_reveal = true

# Internal communication ports
global_manager_port_one = 5552

# Host manager synchronization port
global_manager_pub_port = 5553

# Local host manager port
global_manager_local_pub_port = 5554

# HTTP port for web portal of the host manager
host_manager_http_port = 9300

# Enable worker HTTP servers; each process runs its own server for multi-head
# ingest.
enable_worker_http_servers = true

# Optionally, specify the worker HTTP server ports.
# The default is to use ('head_port' + *rank #*) for each worker process where
# rank number is from "1" to number of ranks in 'rank<#>.host' below.

#rank1.worker_http_server_port = 9192
#rank2.worker_http_server_port = 9193
#rank3.worker_http_server_port = 9194


# Optionally, specify a public URL for each worker HTTP server that clients
# should use to connect for multi-head operations.
#
# NOTE: If specified for any ranks, a public URL must be specified for all
# ranks. Cannot be used in conjunction with nplus1.

#rank0.public_url = 
#rank1.public_url = 
#rank2.public_url = 
#rank3.public_url = 


# Specify the TCP ports each rank will use to communicate with the others.
# If the port for any 'rank<#>' is not specified the port will be assigned to
# 'rank0.communicator_port' + *rank #*.

rank0.communicator_port = 6555
#rank1.communicator_port = 6556
#rank2.communicator_port = 6557
#rank3.communicator_port = 6558

# Enables compression of inter-node network data transfers.
compress_network_data = false


# ==============================================================================
# Security

# Require authentication.
require_authentication = true

# Enable authorization checks.
enable_authorization = false

# Minimum password length.
min_password_length = 0

# Enable external (LDAP, Kerberos, etc.) authentication. User IDs of
# externally-authenticated users must be passed in via the "REMOTE_USER" HTTP
# header from the authentication proxy. May be used in conjuntion with the
# 'enable_httpd_proxy' setting above for an integrated external authentication
# solution.
#
# IMPORTANT: DO NOT ENABLE unless external access to Kinetica ports
# has been blocked via firewall AND the authentication proxy is
# configured to block "REMOTE_USER" HTTP headers passed in from clients.
enable_external_authentication = false

# Encrypted key that, if specified, must be passed in unencrypted via the
# "KINETICA_HANDSHAKE_KEY" HTTP header from the authentication proxy if a
# "REMOTE_USER" HTTP header is also passed in. A missing or incorrect handshake
# key will result in rejection of the request.
external_authentication_handshake_key = U2FsdGVkX199AV0m/CP60DtyuR3as5ZAsE6ULDCIdAiosGevqxzUdbme749IjIhip+40lfUZgxnU+AzoXAAzpA==

# Use a single namespace for internal and external user IDs and role names. If
# false, external user IDs must be prefixed with "@" to differentiate them from
# internal user IDs and role names (except in the "REMOTE_USER" HTTP header,
# where the "@" is omitted).
unified_security_namespace = false

# Automatically create accounts for externally-authenticated users.
# If 'enable_external_authentication' is "false", this setting has no effect.
# Note that accounts are not automatically deleted if users are removed
# from the external authentication provider and will be orphaned.
auto_create_external_users = false

# Automatically add roles passed in via the "KINETICA_ROLES" HTTP header to
# externally-authenticated users. Specified roles that do not exist are
# ignored. If 'enable_external_authentication' is "false", this setting has no
# effect.
#
# IMPORTANT: DO NOT ENABLE unless the authentication proxy is
# configured to block "KINETICA_ROLES" HTTP headers passed in from clients.
auto_grant_external_roles = false

# Comma-separated list of roles to revoke from externally-authenticated
# users prior to granting roles passed in via the "KINETICA_ROLES" HTTP
# header, or "*" to revoke all roles. Preceding a role name with an "!"
# overrides the revocation (e.g. "*,!foo" revokes all roles except "foo").
# Leave blank to disable. If either 'enable_external_authentication' or
# 'auto_grant_external_roles' is "false", this setting has no effect.
auto_revoke_external_roles = 


# ==============================================================================
# External Security

# URL of Ranger REST API.  E.g., https://localhost:6080/
# Leave blank for no Ranger Server
security.external.ranger.url = 

# Name of the service created on the Ranger Server to manage this Kinetica instance
security.external.ranger.service_name = kinetica

# Maximum minutes to hold on to data from Ranger
security.external.ranger.cache_minutes = 60

# The network URI for the ranger_authorizer to start. The URI can be either TCP or IPC.
# TCP address is used to indicate the remote ranger_authorizer which may run at other hosts.
# The IPC address is for a local ranger_authorizer.
#
# Example addresses for remote or TCP servers:
#
##  tcp://127.0.0.1:9293
##  tcp://HOST_IP:9293
#
# Example address for local IPC servers:
#
##  ipc:///tmp/gpudb-ranger-0
security.external.ranger_authorizer.address = ipc://${gaia.temp_directory}/gpudb-ranger-0

# Ranger Authorizer timeout in seconds
security.external.ranger_authorizer.timeout = 120

# Remote debugger port used for the ranger_authorizer. Setting the port to "0"
# disables remote debugging.
#
# NOTE:  Recommended port to use is "5005"
security.external.ranger_authorizer.remote_debug_port = 0


# ==============================================================================
# Auditing
#
# This section controls the request auditor, which will audit all requests
# received by the server in full or in part based on the settings below.
# The output location of the audited requests is controlled via settings in
# the Auditing section of "gpudb_logger.conf".

# Controls whether request auditing is enabled. If set to "true",
# the following information is audited for every request: Job ID, URI, User,
# and Client Address. The settings below control whether additional
# information about each request is also audited. If set to "false", all
# auditing is disabled.
enable_audit = false

# Controls whether HTTP headers are audited for each request.
# If 'enable_audit' is "false" this setting has no effect.
audit_headers = false

# Controls whether the body of each request is audited (in JSON
# format). If 'enable_audit' is "false" this setting has no effect.
#
# NOTE: For requests that insert data records, this setting does not control the
# auditing of the records being inserted, only the rest of the request body; see
# 'audit_data' below to control this.
audit_body = false

# Controls whether records being inserted are audited (in JSON
# format) for requests that insert data records. If either 'enable_audit' or
# 'audit_body' is "false", this setting has no effect.
#
# NOTE: Enabling this setting during bulk ingestion of data will rapidly produce
# very large audit logs and may cause disk space exhaustion; use with caution.
audit_data = false

# Controls whether response information is audited for each request.
# If 'enable_audit' is "false" this setting has no effect.
audit_response = false

# Controls whether the above audit settings can be altered at
# runtime via the /alter/system/properties endpoint. In a secure
# environment where auditing is required at all times, this should be set
# to "true" to lock the settings to what is set in this file.
lock_audit = false


# ==============================================================================
# Licensing

# The license key to authorize running.
license_key = mINVc+nGq36n-NEc6cwnBYCqL-b/LVKG8dK+yI-nvlFF3XyL0Wy-mOaw4bVpxDHpNkYEnKLEXJGsZVRsuJhO


# ==============================================================================
# Processes and Threads

# Set min number of web server threads to spawn. (default: "8")
min_http_threads = 8

# Set max number of web server threads to spawn. (default: "512")
max_http_threads = 512

# Set the maximum number of threads per tom for table initialization on gpudb startup
init_tables_num_threads_per_tom = 8

# Set the maximum number of threads (both workers and masters) to be passed
# to TBB on initialization.  Generally speaking,
# 'max_tbb_threads_per_rank' - "1" TBB workers will be created.  Use "-1" for no
# limit.
max_tbb_threads_per_rank = -1

# Set the number of TOMs (data container shards) per rank
toms_per_rank = 1

# Size of the worker rank data thread pool. This includes operations such as
# inserts, updates, and deletes on table data.
# NB multi-head inserts are not affected by this limit.
tps_per_tom = -1

# Size of the worker rank processing thread pool. This is primarily used for
# computation-based operations such as aggregates and record retrieval.
tcs_per_tom = -1

# Specifies the number of concurrent queries to prioritize at once in the task scheduler.
# Used to control depth-first query execution (execute the current query as quickly as possible)
# versus breadth-first (execute as many queries as possible).
# This is not a hard limit. If the total number of available queries exceeds this value and
# there are idle threads, the other, i.e. non-prioritized, queries will be executed.
subtask_concurrency_limit = -1

# ==============================================================================
# Hardware

# Specify the GPU to use for all calculations on the HTTP server node,
# **rank0**.
#
# NOTE: The **rank0** GPU may be shared with another rank.
rank0.gpu = 0

# Set the GPU device for each worker rank to use. If no GPUs are specified,
# each rank will round-robin the available GPUs per host system.
# Add 'rank<#>.taskcalc_gpu' as needed for the worker ranks, where *#* ranges
# from "1" to the highest *rank #* among the 'rank<#>.host' parameters
#
# Example setting the GPUs to use for ranks 1 and 2:
#
##   rank1.taskcalc_gpu = 0
##   rank2.taskcalc_gpu = 1

rank1.taskcalc_gpu = 
rank2.taskcalc_gpu = 
rank3.taskcalc_gpu = 


# Set the head HTTP **rank0** numa node(s).
# If left empty, there will be no thread affinity or preferred memory node.
# The node list may be either a single node number or a range; e.g., "1-5,7,10".
#
# If there will be many simultaneous users, specify as many nodes as possible
# that won't overlap the **rank1** to **rankN** worker numa nodes that the GPUs
# are on.
#
# If there will be few simultaneous users and WMS speed is important, choose
# the numa node the 'rank0.gpu' is on.

rank0.numa_node = 0 

# Set each worker rank's preferred base numa node for CPU affinity and memory
# allocation. The 'rank<#>.base_numa_node' is the node or nodes that non-data
# intensive threads will run in. These nodes do not have to be the same numa
# nodes that the GPU specified by the corresponding 'rank<#>.taskcalc_gpu' is on
# for best performance, though they should be relatively near to their
# 'rank<#>.data_numa_node'.
#
# There will be no CPU thread affinity or preferred node for memory allocation
# if not specified or left empty.
#
# The node list may be a single node number or a range; e.g., "1-5,7,10".

rank1.base_numa_node = 0
rank2.base_numa_node = 1
rank3.base_numa_node = 2

# Set each worker rank's preferred data numa node for CPU affinity and memory
# allocation. The 'rank<#>.data_numa_node' is the node or nodes that data
# intensive threads will run in and should be set to the same numa node that the
# GPU specified by the corresponding 'rank<#>.taskcalc_gpu' is on for best
# performance.
#
# If the 'rank<#>.taskcalc_gpu' is specified the 'rank<#>.data_numa_node' will
# be automatically set to the node the GPU is attached to, otherwise
# there will be no CPU thread affinity or preferred node for memory allocation
# if not specified or left empty.
#
# The node list may be a single node number or a range; e.g., "1-5,7,10".

rank1.data_numa_node = 0
rank2.data_numa_node = 1
rank3.data_numa_node = 2

# ==============================================================================
# General

# Time-to-live in minutes of non-protected tables before they are automatically
# deleted from the database.
default_ttl = 20

# Disallow the /clear/table request to clear all tables.
disable_clear_all = true

# Size in bytes of the pinned memory pool per-rank process to speed up copying
# data to the GPU.  Set to "0" to disable.
pinned_memory_pool_size = 8589934592

# Enable (if "true") multiple kernels to run concurrently on the same GPU
concurrent_kernel_execution = true

# Maximum number of kernels that can be running at the same time on a given
# GPU. Set to "0" for no limit. Only takes effect if
# 'concurrent_kernel_execution' is "true"
max_concurrent_kernels = 0

# If "true" then all filter execution will be host-only (i.e. CPU). This can be useful
# for high-concurrency situations and when PCIe bandwidth is a limiting factor.
force_host_filter_execution = false

# Maximum number of records that data retrieval requests such as
# /get/records and /aggregate/groupby will return per request.
max_get_records_size = 20000

# Timeout (in minutes) for filter-type requests
request_timeout = 20

# Set an optional executable command that will be run once when Kinetica is
# ready for client requests. This can be used to perform any initialization
# logic that needs to be run before clients connect. It will be run as the
# "gpudb" user, so you must ensure that any required permissions are set on the
# file to allow it to be executed.  If the command cannot be executed or returns
# a non-zero error code, then Kinetica will be stopped.  Output from the startup
# script will be logged to "/opt/gpudb/core/logs/gpudb-on-start.log" (and its
# dated relatives).  The "gpudb_env.sh" script is run directly before the
# command, so the path will be set to include the supplied Python runtime.
#
# Example:
#
## on_startup_script = /bin/ks arg1 arg2 ...
on_startup_script = 

# Enable predicate-equi-join filter plan type
enable_predicate_equi_join = true

# Enable overlapped-equi-join filters
enable_overlapped_equi_join = true

# Timeout (in seconds) to wait for each database subsystem to startup.
# Subsystems include the Query Planner, Graph, Stats, & HTTP servers, as well
# as external text-search ranks.
timeout_startup_subsystem = 60

# Timeout (in seconds) to wait for each database subsystem to exit gracefully
# before it is force-killed.
timeout_shutdown_subsystem = 20

# Timeout (in seconds) to wait for a rank to start during a cluster event (ex: failover)
# event is considered failed.
cluster_event_timeout_startup_rank = 300

# Timeout (in seconds) to wait for a rank to exit gracefully before it is
# force-killed. Machines with slow disk drives may require longer times and data
# may be lost if a drive is not responsive.
timeout_shutdown_rank = 300


# ==============================================================================
# Visualization
#
# Several of these options interact when determining the system resources
# required for visualization.  Worker ranks use "F(N+1)" bytes of GPU memory
# (VRAM) and "F" bytes of main memory (RAM) where:
#
## F = max_heatmap_size * max_heatmap_size * 6 bytes
## N = opengl_antialiasing_level
#
# For example, when 'max_heatmap_size' is "3072", and 'opengl_antialasing_level'
# is "0", 56.6 MB of VRAM are required.  When 'opengl_antialasing_level' is "4",
# 283 MB are required.

# Threshold number of points (per-TOM) at which point rendering switches to fast mode.
point_render_threshold = 100000

# Threshold for the number of points (per-TOM) after which symbology rendering
# falls back to regular rendering
symbology_render_threshold = 10000

# Maximum heatmap size (in pixels) that can be generated. This reserves
# 'max_heatmap_size' ^ 2 * 8 bytes of GPU memory at **rank0**
max_heatmap_size = 3072

# The image width/height (in pixels) of svg symbols cached in the OpenGL symbol cache.
symbol_resolution = 100

# The width/height (in pixels) of an OpenGL texture which caches symbol images for OpenGL rendering.
symbol_texture_size = 4000

# If "true", enable hardware-accelerated OpenGL renderer; if "false", use the
# software-based Cairo renderer.
enable_opengl_renderer = true

# The number of samples to use for antialiasing. Higher numbers will improve
# image quality but require more GPU memory to store the samples on worker
# ranks.  This affects only the OpenGL renderer.
#
# Value may be "0", "4", "8" or "16".  When "0" antialiasing is disabled.
# The default value is "0".
opengl_antialiasing_level = 0

# Single-precision coordinates are used for usual rendering processes, but depending on
# the precision of geometry data and use case, double precision processing may be required
# at a high zoom level. Double precision rendering processes are used from the zoom level
# specified by this parameter, which is corresponding to a zoom level of TMS or Google map service.
rendering_precision_threshold = 30

# Enable level-of-details rendering for fast interaction with large WKT polygon
# data.  Only available for the OpenGL renderer (when 'enable_opengl_renderer'
# is "true").
enable_lod_rendering = true

# If "true", enable Vector Tile Service (VTS) to support client-side visualization
# of geospatial data. Enabling this option increases memory usage on ingestion.
enable_vectortile_service = false

# Input geometries are pre-processed upon ingestion for faster vector tile generation.
# This parameter determines the zoom level from which the vector tile pre-processing starts.
# A vector tile request for a lower zoom level than this parameter takes additional time
# because the vector tile needs to be generated on the fly.
min_vectortile_zoomlevel = 1

# Input geometries are pre-processed upon ingestion for faster vector tile generation.
# This parameter determines the zoom level at which the vector tile pre-processing stops.
# A vector tile request for a higher zoom level than this parameter takes additional time
# because the vector tile needs to be generated on the fly.
max_vectortile_zoomlevel = 4

# The name of map tiler used for Vector Tile Service.
# "google" and "tms" map tilers are supported currently.
# This parameter should be matched with the map tiler of clients' vector tile renderer.
vectortile_map_tiler = google

# Longitude and latitude ranges of geospatial data for which level-of-details
# representations are being generated. The parameter order is:
#
## <min_longitude> <min_latitude> <max_longitude> <max_latitude>
#
# The default values span over the world, but the level-of-details rendering
# becomes more efficient when the precise extent of geospatial data is
# specified.
lod_data_extent = -180 -90 180 90

# The number of subregions in horizontal and vertical geospatial data extent.
# The default values of "12 6" divide the world into subregions of
# 30 degree (lon.) x 30 degree (lat.)
lod_subregion_num = 12 6

# A base image resolution (width and height in pixels) at which a subregion
# would be rendered in a global view spanning over the whole dataset. Based on
# this resolution level-of-details representations are generated for the
# polygons located in the subregion.
lod_subregion_resolution = 512 512

# The maximum number of levels in the level-of-details rendering. As the number
# increases, level-of-details rendering becomes effective at higher zoom levels,
# but it may increase memory usage for storing level-of-details representations.
max_lod_level = 8

# The extent to which shape data are pre-processed for level-of-details
# rendering during data insert/load or processed on-the-fly in rendering time.
# This is a trade off between speed and memory. The higher the value, the faster
# level-of-details rendering is, but the more memory is used for storing
# processed shape data.
#
# The maximum level is "10" (most shape data are pre-processed) and the minimum
# level is "0".
lod_preprocessing_level = 5


# ==============================================================================
# Video

# System default TTL for videos. 
# Time-to-live (TTL) is the number of minutes before a video will expire and be 
# removed, or -1 to disable.
video_default_ttl = -1

# The maximum number of videos to allow on the system. Set to 0 to disable video 
# rendering.  Set to -1 to allow an unlimited number of videos.
video_max_count = -1

# Directory where video files should be temporarily stored while rendering.
# Only accessed by rank 0.
video_temp_directory = ${gaia.temp_directory}/gpudb-temp-videos


# ==============================================================================
# GAdmin
#
# GAdmin is a web-based UI for administering the Kinetica DB and system.

# Enable Tomcat web-server for the GAdmin UI
enable_tomcat = true


# ==============================================================================
# Workbench
#
# Workbench is a web-based UI for managing database objects and writing rich SQL
# workbooks in Kinetica.

# Start the Workbench app on the head host when host manager is started.
enable_workbench = true

# HTTP server port for Workbench if enabled.
workbench_port = 8000


# ==============================================================================
# Text Search

# Enable text search capability within the database.
enable_text_search = false

# Use the production capable external text server instead of a lightweight
# internal server which should only be used for light testing.
#
# NOTE: The internal text server is deprecated and may be removed in future versions.
use_external_text_server = true

# Number of text indices to start for each rank
text_indices_per_tom = 2

# Searcher refresh intervals - specifies the maximum delay (in
# seconds) between writing to the text search index and being able to
# search for the value just written.  A value of "0" insures that writes
# to the index are immediately available to be searched.  A more
# nominal value of "100" should improve ingest speed at the cost
# of some delay in being able to text search newly added values.
text_searcher_refresh_interval = 20

# External text server addresses to use if 'use_external_text_server' is "true".
#
# Specify one for each worker 'rank<#>', where *#* ranges from "1" to highest
# index in 'rank<#>.host'. Add the appropriate number of
# 'rank<#>.text_index_address' for each worker rank as needed. The addresses can
# be a fully qualified TCP *address:port* for remote servers or an IPC address
# for local text index servers.
#
# If no addresses are specified, the text index servers will use IPC and be
# started on the machine where the rank is running as shown in the IPC example
# below. You should either specify all addresses or none to get the defaults.
#
# Example for remote or TCP servers:
#
##  rank1.text_index_address  = tcp://127.0.0.1:4000
##  rank2.text_index_address  = tcp://127.0.0.1:4001
##  ... up to rank<N>.text_index_address = ...
#
# Example for local IPC servers:
#
##  rank1.text_index_address  = ipc:///tmp/gpudb-text-index-1
##  rank2.text_index_address  = ipc:///tmp/gpudb-text-index-2
##  ... up to rank<N>.text_index_address = ...
#
# Where "/tmp/gpudb-text-index-1" is the name of the socket file to create.

rank1.text_index_address  = ipc://${gaia.temp_directory}/gpudb-text-index-1
rank2.text_index_address  = ipc://${gaia.temp_directory}/gpudb-text-index-2
rank3.text_index_address  = ipc://${gaia.temp_directory}/gpudb-text-index-3


# ==============================================================================
# Persistence

# Specify a base directory to store persistence data files.
persist_directory = /opt/gpudb/persist

# Base directory to store hashed strings.
sms_directory = ${gaia.persist_directory}

# Base directory to store the text search index.
text_index_directory = ${gaia.persist_directory}

# Directory for Kinetica to use to store temporary files.
# Must be a fully qualified path, have at least 100Mb of free space, and execute
# permission.
temp_directory = ${gaia.persist_directory}/tmp

# Fsync directories in persist after every update to ensure filesystem toc is up to date.
fsync_inodes_immediate = false

# Maximum number of open metadata stores per rank.
metadata_store_pool_size = 128

# Sync mode to use when persisting metadata stores to disk:
#
# * "off"       : Turn off synchronous writes by default
# * "normal"    : Use normal synchronous writes by default
# * "full"      : Use full synchronous writes by default
metadata_store_sync_mode = normal

# Maximum size of each wal segment file. Larger sizes generally require more objects to be flushed
# before a given segment can be freed thus increasing the wal disk usage whereas smaller sizes
# increase the number of disk open/close operations.
wal.max_segment_size = 500000000

# Approximate number of segment files to split the wal across. A minimum of two is required.
# The size of the wal is limited by segment_count * max_segment_size. (per rank and per tom)
# Set to 0 to remove a size limit on the wal itself, but still be bounded by rank tier limits.
# Set to -1 to have the database decide automatically per table.
wal.segment_count = -1

# Sync mode to use when persisting wal entries to disk:
#
# * "none"       : Disable the wal
# * "background" : Wal entries are periodically flushed instead of immediately after each operation
# * "flush"      : Protects entries in the event of a database crash
# * "fsync"      : Protects entries in the event of an OS crash
wal.sync_policy = flush

# Given a sync_policy of 'background', specifies how frequently wal entries are flushed to disk
wal.flush_frequency = 60

# Enable checksum protection on the wal entries
wal.checksum = true

# If true, any table that is found to be corrupt after replaying its wal at startup will
# automatically be truncated so that the table becomes operable. If false, the user will be
# responsible for resolving the issue via sql REPAIR TABLE or similar.
wal.truncate_corrupt_tables_on_start = true

# Compress data in memory immediately (or in the background) when compression is
# applied to an existing table's column(s) via /alter/table
synchronous_compression = false

# The maximum time in seconds a secondary column store persist data file can be out of
# sync with memory. Set to a very high number to disable forced syncing.
persist_sync_time = 5

# Startup data-loading scheme:
#
# * "always"    : load as much of the stored data as possible into memory before
#                 accepting requests
# * "lazy"      : load the necessary data to start, and load as much of the
#                 remainder of the stored data as possible into memory lazily
# * "on_demand" : only load data as requests use it
load_vectors_on_start = on_demand
build_pk_index_on_start = always
build_materialized_views_on_start = always

# Maximum number of active automatic view updators
max_auto_view_updators = 3

# Maximum number of open files (per-TOM) for the SMS (string) store
sms_max_open_files = 128

# Number of records per chunk ("0" disables chunking)
chunk_size = 8000000

# Target maximum data size for any one column in a chunk (512 MB) (0 = disable)
chunk_column_max_memory = 512000000

# Target maximum total data size for all columns in a chunk (8 GB) (0 = disable)
chunk_max_memory = 8192000000

# Determines whether to execute kernels on host (CPU) or device (GPU). Possible values are:
#
# * "default"   : engine decides
# * "host"      : execute only host
# * "device"    : execute only device
# * *<rows>*    : execute on the host if chunked column contains the given
#                 number of *rows* or fewer; otherwise, execute on device.
execution_mode = device

# Whether or not to enable chunk caching
shadow_cube_enabled = true

# The maximum number of bytes in the shadow aggregate cache
shadow_agg_size = 500000000

# The maximum number of bytes in the shadow filter cache
shadow_filter_size = 500000000

# ==============================================================================
# Monitoring / Statistics

# Run a statistics server to collect information about Kinetica and the machines
# it runs on.
enable_stats_server = true

# Use the internal event and metric server on the head host server
# if true, otherwise use the Kagent services.
# Note that Kagent installs will automatically set this value to false.
event_server_internal = false

# Event collector server address and port.
event_server_address = ${gaia.host0.address}
event_server_port = 9080

# Enable running promtail to parse and send logs to the event_server_address.
enable_promtail = false

# Alertmanager server address and port.
alertmanager_address = ${gaia.event_server_address}
alertmanager_port = 9089

# Fluentbit TCP server address and port with a json format parser.
fluentbit_address = 
fluentbit_port = 5170

# Store query metrics in a persistent table. If disabled metrics will still be
# available for point-in-time export.
telm.persist_query_metrics = true

# ==============================================================================
# Procs

# Enable procs (UDFs)
enable_procs = false

# Directory where proc files are stored at runtime. Must be a fully qualified
# path with execute permission. Python virtual environment installations are
# also persisted in this directory and can be referenced by executing procs.
proc_directory = /opt/gpudb/procs

# Directory where data transferred to and from procs is written. Must be a
# fully qualified path with sufficient free space for required volume of data.
proc_data_directory = /opt/gpudb/procs


# ==============================================================================
# Graph Servers
#
# One or more graph servers can be created across available hosts.

# ------------------------------------------------------------------------------
# Global Parameters

# Enable/Disable all graph operations
enable_graph_server = false

# Port used for responses from the graph server to the database server.
graph.head_port = 8100

# ------------------------------------------------------------------------------
# Server-Specific Parameters
#
# Any number of graph servers may be configured, starting with 'server0', and
# must be specified in consecutive order (e.g., 'server0', 'server1', 'server2').
#
# Server settings are defined as follows:
## graph.server<#>.<parameter>

# Valid *parameter* names include:
#
# * host        : Host on which the graph server process will be run.
# * port        : Ports used for requests from the database server to the graph 
#                 server(s).
#                 Either all ports (one per graph server) should be specified or  
#                 a single starting value that will be incremented for each 
#                 subsequent graph.
# * ram_limit   : Maximum RAM memory (bytes) a server can use at any given time 
#                 Default is "0", which uses rank ram tier limits to limit memory
#                 See the Tiered Storage section for tier limit details
#
# Example of two graph servers, server1 has a specified RAM limit:
#
## graph.head_port = 8099
## graph.server0.host = host2
## graph.server1.host = host4
## graph.server0.ram_limit = 0
## graph.server1.ram_limit = 500000000

graph.server0.host = host0
graph.server0.ram_limit = 0

# ==============================================================================
# Etcd

# List of accessible etcd server URLs.
etcd_urls = 

# Encrypted login credential for Etcd at given URLs.
etcd_auth_user = 

# ==============================================================================
# HA
#
# Enable/Disable HA from here. All other parameters will be in etcd at the
# address specified in the relevant section above.

# Enable HA.
enable_ha = false

# ==============================================================================
# Machine Learning (ML)

# Enable the ML server.
enable_ml = false

# default ML API service port number
ml_api_port = 9187

# ==============================================================================
# Alerts

# Enable the alerting system.
enable_alerts = true

# Executable to run when an alert condition occurs. This executable will
# only be run on **rank0** and does not need to be present on other nodes.
alert_exe = 

# Trigger an alert whenever the status of a host or rank changes.
alert_host_status = true

# Optionally, filter host alerts for a comma-delimited list of statuses.
# If a filter is empty, every host status change will trigger an alert.
alert_host_status_filter = fatal_init_error

# Trigger an alert whenever the status of a rank changes.
alert_rank_status = true

# Optionally, filter rank alerts for a comma-delimited list of statuses.
# If a filter is empty, every rank status change will trigger an alert.
alert_rank_status_filter = fatal_init_error, not_responding, terminated

# Trigger an alert if a CUDA error occurs on a rank.
alert_rank_cuda_error = true

# Trigger alerts when the fallback allocator is employed; e.g., host memory is
# allocated because GPU allocation fails.
#
# NOTE:  To prevent a flooding of alerts, if a fallback allocator is triggered
# in bursts, not every use will generate an alert.
alert_rank_fallback_allocator = true

# Trigger generic error message alerts, in cases of various significant runtime
# errors.
alert_error_messages = true

# Trigger an alert if available memory on any given node falls to or below a
# certain threshold, either absolute (number of bytes) or percentage of total
# memory. For multiple thresholds, use a comma-delimited list of values.
alert_memory_percentage = 20, 10, 5, 1
alert_memory_absolute = 

# Trigger an alert if available disk space on any given node falls to or below a
# certain threshold, either absolute (number of bytes) or percentage of total
# disk space. For multiple thresholds, use a comma-delimited list of values.
alert_disk_percentage = 20, 10, 5, 1
alert_disk_absolute = 

# The maximum number of triggered alerts guaranteed to be stored at any given
# time. When this number of alerts is exceeded, older alerts may be discarded to
# stay within the limit.
alert_max_stored_alerts = 100

# Directory where the trace event and summary files are stored.  Must be a
# fully qualified path with sufficient free space for required volume of data.
trace_directory = ${gaia.temp_directory}

# The maximum number of trace events to be collected
trace_event_buffer_size = 1000000

# ==============================================================================
# Failover (NPlus1)

# Whether or not the system should fail over failed processes on the head/worker
# nodes to other nodes
#
# * "true"  : Initiate failover to another node if a process fails
# * "false" : Do not initiate failover to another node (restart on the same node)
np1.enable_head_failover = false
np1.enable_worker_failover = false

# Controls the failure threshold for processes before failover is attempted.
# A process must exceed its restart attempts limit within the restart interval.
# Number of successive times a process will be restarted
np1.rank_restart_attempts = 1
np1.critical_restart_attempts = 1
np1.non_critical_restart_attempts = 3

# Time interval in seconds since last restart after which restart count will be reset
np1.restart_interval = 60

# An api script is required for persistent storage which supports dynamic mounting, such as cloud-based storage.
# This script is responsible for issuing the underlying api calls directly based on requests from the database.
# If this field is blank it implies all hosts will have access to persist for all ranks at all times.
np1.storage_api_script = 

# Enable concurrent mounting operations. By default, multiple attach or detach
# disk volume operations are permitted to run concurrently. Set to 'false' to limit to
# running a single volume attach/detach operation on a host at a time.
np1.enable_concurrent_mount_ops = true

# Post-migration startup data-loading scheme:
#
# * "always"    : load as much of the stored data as possible into memory before
#                 accepting requests
# * "lazy"      : load the necessary data to start, and load as much of the
#                 remainder of the stored data as possible into memory lazily
# * "on_demand" : only load data as requests use it
np1.load_vectors_on_migration = always
np1.build_pk_index_on_migration = always
np1.build_materialized_views_on_migration = always

# Heartbeats are used to detect host-level failures.
# The frequency in seconds that heartbeat requests are broadcasted to hosts
heartbeat_interval = 20

# The allowable window in seconds for heartbeat responses (must be less than heartbeat_interval)
heartbeat_timeout = 10

# The number of allowable consecutive missed heartbeats before the host is considered
# degraded and any failover events begin
heartbeat_missed_limit = 3

# ==============================================================================
# Postgres Proxy Server
# Start an Postgres(TCP) server as a proxy to handle postgres wire protocol messages.
enable_postgres_proxy = true

# TCP port that the postgres  proxy server will listen on if
# 'enable_postgres_proxy' is "true".
postgres_proxy.port = 5432

# Set min number of server threads to spawn. (default: "2")
postgres_proxy.min_threads = 2

# Set max number of server threads to spawn. (default: "64")
postgres_proxy.max_threads = 64

# Set max number of queued server connections. (default: "1")
postgres_proxy.max_queued_connections = 1

# Set idle connection  timeout in seconds. (default: "300")
postgres_proxy.idle_connection_timeout = 300

# Keep connections alive between requests
postgres_proxy.keep_alive = false

# Set to "true" to use SSL; if "true" then 'ssl_key_file' and
# 'ssl_cert_file' must be provided
postgres_proxy.ssl = false

# Files containing the SSL private Key and the SSL certificate for.
# If required, a self signed certificate (expires after 10 years) can be
# generated via the command:
#
## openssl req -newkey rsa:2048 -new -nodes -x509 \
##   -days 3650 -keyout key.pem -out cert.pem
#
postgres_proxy.ssl_key_file = 
postgres_proxy.ssl_cert_file = 

# ==============================================================================
# SQL Engine

# Enable Query Planner
sql.enable_planner = true

# The network URI for the query planner to start. The URI can be either TCP or IPC.
# TCP address is used to indicate the remote query planner which may run at other hosts.
# The IPC address is for a local query planner.
#
# Example for remote or TCP servers:
#
##  sql.planner.address  = tcp://127.0.0.1:9293
##  sql.planner.address  = tcp://HOST_IP:9293
#
# Example for local IPC servers:
#
##  sql.planner.address  = ipc:///tmp/gpudb-query-engine-0
sql.planner.address = ipc://${gaia.temp_directory}/gpudb-query-engine-0

# Remote debugger port used for the query planner. Setting the port to "0"
# disables remote debugging.
#
# NOTE:  Recommended port to use is "5005"
sql.planner.remote_debug_port = 0

# The maximum memory for the query planner to use in Megabytes.
sql.planner.max_memory = 4096

# The maximum stack size for the query planner threads to use in Megabytes.
sql.planner.max_stack = 6

# Query planner timeout in seconds
sql.planner.timeout = 120

# Max Query planner threads
sql.planner.workers = 16

# Enable query results caching
sql.results.caching = true

# TTL of the query cache results table
sql.results.cache_ttl = 60

# Perform joins between only 2 tables at a time;
# default is all tables involved in the operation at once
sql.force_binary_joins = false

# Perform unions/intersections/exceptions between only 2 tables at a time;
# default is all tables involved in the operation at once
sql.force_binary_set_ops = false

# The maximum number of entries in the SQL plan cache.  The default is "4000"
# entries, but the configurable range is "1" - "1000000".  Plan caching will be
# disabled if the value is set outside of that range.
sql.plan_cache_size = 4000

# Maximum number of entries in the SQL result cache.  The default is "4000"
sql.result_cache_size = 4000

# Enable rule-based query rewrites
sql.rule_based_optimization = true

# Enable the cost-based optimizer
sql.cost_based_optimization = true

# Enable distributed joins
sql.distributed_joins = true

# Enable distributed operations
sql.distributed_operations = true

# Enable parallel query evaluation
sql.parallel_execution = true

# Max parallel steps
sql.max_parallel_steps = 4

# TTL of the paging results table
sql.paging_table_ttl = 20

# Max allowed view nesting levels. Valid range ("1"-"64")
sql.max_view_nesting_levels = 16

# Max Query metadata helper threads
sql.metadata_workers = 16

# ==============================================================================
# AI

# AI API provider type.
ai.api.provider = sqlgpt

# AI API URL. The default is "https://sqlgpt.io/api/sql/suggest"
ai.api.url = 

# AI API key.
ai.api.key = 

# AI API connection timeout in seconds
ai.api.connection_timeout = 60

# ==============================================================================
# External Files

# Defines the directory from which external files can be loaded
external_files_directory = ${gaia.persist_directory}

# Maximum number of simultaneous threads allocated to a given external file read
# request, on each rank. Note that thread allocation may also be limited by
# resource group limits, or system load.
external_file_reader_num_tasks = -1

# Max file size (in MB) to allow saving to a single file.
# May be overridden by target limitations.
egress_single_file_max_size = 10000

# Parquet files compression type
egress_parquet_compression = snappy

# Maximum number of records to be ingested in a single batch
kafka.batch_size = 1000

# Maximum wait time (seconds) to buffer records received from kafka before ingestion
kafka.wait_time = 30

# Maximum time (milliseconds) for each poll to get records from kafka
kafka.poll_timeout = 0

# System metadata catalog settings
system_metadata.stats_retention_days = 21
system_metadata.stats_aggr_rowcount = 10000
system_metadata.stats_aggr_time = 1

# For persistent metadata tables, time in seconds to retain rows prior to deletion. NB records are
# deleted periodically so the retention_period is the minimum lifetime of a given record.
system_metadata.retention_period = 604800

# ==============================================================================
# Tiered Storage
#
# Defines system resources using a set of containers (tiers) in which data can
# be stored, either on a temporary (memory) or permanent (disk) basis. Tiers
# are defined in terms of their maximum capacity and water mark thresholds.
# The general format for defining tiers is:
#
## tier.<tier_type>.<config_level>.<parameter>
#
# where *tier_type* is one of the five basic types of tiers:
#
# * 'vram'    : GPU memory
# * 'ram'     : Main memory
# * 'disk'    : Disk cache
# * 'persist' : Permanent storage
# * 'cold'    : Extended long-term storage
#
# Each tier can be configured on a global or per-rank basis, indicated by the
# *config_level*:
#
# * 'default' : global, applies to all ranks, must be accessible by all hosts
# * 'rank<#>' : local, applies only to the specified rank, overriding any global
#               default
#
# If a field is not specified at the 'rank<#>' level, the specified 'default'
# value applies. If neither is specified, the global system defaults will take
# effect, which vary by tier.
#
# The parameters are also tier-specific and will be listed in their respective
# sections, though every tier, except Cold Storage, will have the following:
#
# * 'limit'          : [ "-1", "1" .. "N" ] (bytes)
# * 'high_watermark' : [ "1" .. "100" ] (percent)
# * 'low_watermark'  : [ "1" .. "100" ] (percent)
#
# NOTE:  To disable watermark-based eviction, set the 'low_watermark' and
# 'high_watermark' values to "100". Watermark-based eviction is also ignored
# if the tier limit is set to -1 (no limit).

# ------------------------------------------------------------------------------
# Global Tier Parameters

# Timeout in seconds for subsequent requests to wait on a locked resource
tier.global.concurrent_wait_timeout = 600

# If true, the tier manager will prioritize the eviction of persistent objects
# within a given tier priority.
tier.global.defer_cache_object_evictions_to_disk = true

# ------------------------------------------------------------------------------
# VRAM Tier
#
# The VRAM Tier is composed of the memory available in one or multiple GPUs per
# host machine.
#
# A default memory limit and eviction thresholds can be set for CUDA-enabled
# devices across all ranks, while one or more ranks may be configured to
# override those defaults.
#
# The general format for VRAM settings:
#
## tier.vram.[default|rank<#>].all_gpus.<parameter>

# Valid *parameter* names include:
#
# * 'limit'          : The maximum VRAM (bytes) per rank that can be allocated
#                      on GPU(s) across all resource groups.  Default is "-1",
#                      signifying to reserve 95% of the available GPU memory at
#                      startup.
# * 'high_watermark' : VRAM percentage used eviction threshold.  Once memory
#                      usage exceeds this value, evictions from this tier will be
#                      scheduled in the background and continue until the
#                      'low_watermark' percentage usage is reached.  Default is
#                      "90", signifying a 90% memory usage threshold.
# * 'low_watermark'  : VRAM percentage used recovery threshold.  Once memory
#                      usage exceeds the 'high_watermark', evictions will
#                      continue until memory usage falls below this recovery
#                      threshold. Default is "80", signifying an 80% memory usage
#                      threshold.
#
tier.vram.default.all_gpus.limit = -1
tier.vram.default.all_gpus.high_watermark = 90
tier.vram.default.all_gpus.low_watermark = 80


# ------------------------------------------------------------------------------
# RAM Tier
#
# The RAM Tier represents the RAM available for data storage per rank.
#
# The RAM Tier is NOT used for small, non-data objects or variables that are
# allocated and deallocated for program flow control or used to store metadata or
# other similar information; these continue to use either the stack or the regular
# runtime memory allocator. This tier should be sized on each machine such that
# there is sufficient RAM left over to handle this overhead, as well as the needs
# of other processes running on the same machine.
#
# A default memory limit and eviction thresholds can be set across all ranks,
# while one or more ranks may be configured to override those defaults.
#
# If any ranks on a host has the default memory limit ("-1"), their maximum
# allocation will be calculated as follows:
#
# * Head Node
#       Rank0: 10% of system memory
#       Other ranks: 70% of system memory / # worker ranks
# * Worker Node
#       Worker ranks: 80% of system memory / # worker ranks
#
# The general format for RAM settings:
#
## tier.ram.[default|rank<#>].<parameter>

# Valid *parameter* names include:
#
# * 'limit'          : The maximum RAM (bytes) per rank that can be allocated
#                      across all resource groups.  Default is "-1", signifying
#                      to automatically set the maximum capacity as a portion of
#                      total system memory or the host limit.
# * 'high_watermark' : RAM percentage used eviction threshold.  Once memory
#                      usage exceeds this value, evictions from this tier will be
#                      scheduled in the background and continue until the
#                      'low_watermark' percentage usage is reached.  Default is
#                      "90", signifying a 90% memory usage threshold.
# * 'low_watermark'  : RAM percentage used recovery threshold.  Once memory
#                      usage exceeds the 'high_watermark', evictions will
#                      continue until memory usage falls below this recovery
#                      threshold. Default is "80", signifying an 80% memory usage
#                      threshold.
#
tier.ram.default.limit = -1
tier.ram.default.high_watermark = 90
tier.ram.default.low_watermark = 80

tier.ram.rank0.limit = 15000000000

tier.ram.rank1.limit = 180352900000

tier.ram.rank2.limit = 180353599999

tier.ram.rank3.limit = 180359199999


# ------------------------------------------------------------------------------
# Disk Tier
#
# Disk Tiers are used as temporary swap space for data that doesn't fit in RAM
# or VRAM. The disk should be as fast or faster than the Persist Tier storage
# since this tier is used as an intermediary cache between the RAM and Persist
# Tiers. Multiple Disk Tiers can be defined on different disks with different
# capacities and performance parameters. No Disk Tiers are required, but they
# can improve performance when the RAM Tier is at capacity.
#
# A default storage limit and eviction thresholds can be set across all ranks
# for a given Disk Tier, while one or more ranks within a Disk Tier may be
# configured to override those defaults.
#
# The general format for Disk settings:
#
## tier.disk<#>.[default|rank<#>].<parameter>
#
# Multiple Disk Tiers may be defined such as 'disk', 'disk0', 'disk1', ... etc.
# to support different tiering strategies that use any one of the Disk Tiers.  A
# tier strategy can have, at most, one Disk Tier.  Create multiple tier
# strategies to use more than one Disk Tier, one per strategy.  See
# 'tier_strategy' parameter for usage.

# Valid *parameter* names include:
#
# * 'path'           : A base directory to use as a swap space for this tier.
# * 'limit'          : The maximum disk usage (bytes) per rank for this tier
#                      across all resource groups.  Default is "-1", signifying
#                      no limit and ignore watermark settings.
# * 'high_watermark' : Disk percentage used eviction threshold.  Once disk
#                      usage exceeds this value, evictions from this tier will be
#                      scheduled in the background and continue until the
#                      'low_watermark' percentage usage is reached.  Default is
#                      "90", signifying a 90% disk usage threshold.
# * 'low_watermark'  : Disk percentage used recovery threshold.  Once disk usage
#                      exceeds the 'high_watermark', evictions will continue
#                      until disk usage falls below this recovery threshold.
#                      Default is "80", signifying a 80% disk usage threshold.
# * 'store_persistent_objects' :
#                      If true, allow the disk cache to store copies of data even if
#                      they are already stored in a persistent tier (persist/cold).
#
# Example 'default' disk cache configuration using 'disk0':
#
## tier.disk0.default.path = /opt/gpudb/diskcache_0
## tier.disk0.default.limit = -1
## tier.disk0.default.high_watermark = 90
## tier.disk0.default.low_watermark = 80
## tier.disk0.default.store_persistent_objects = false

tier.disk0.default.path = ${gaia.persist_directory}/diskcache
tier.disk0.default.limit = -1
tier.disk0.default.high_watermark = 90
tier.disk0.default.low_watermark = 80
tier.disk0.default.store_persistent_objects = false


# ------------------------------------------------------------------------------
# Persist Tier
#
# The Persist Tier is a single pseudo-tier that contains data in persistent form
# that survives between restarts. Although it also is a disk-based tier, its
# behavior is different from Disk Tiers:  data for persistent objects is always
# present in the Persist Tier (or Cold Storage Tier, if configured), but may not
# be up-to-date at any given time.
#
# A default storage limit and eviction thresholds can be set across all ranks,
# while one or more ranks may be configured to override those defaults.  The
# Graph Solver engine may be given its own storage settings; however, it is not
# tiered, and therefore cannot have limit/watermark settings applied.
#
# The general format for Persist settings:
#
## tier.persist.[default|rank<#>|text<#>|graph<#>].<parameter>
#
# WARNING:  In general, limits on the Persist Tier should only be set if one or
# more Cold Storage Tiers are configured.  Without a supporting Cold Storage
# Tier to evict objects in the Persist Tier to, operations requiring space in
# the Persist Tier will fail when the limit is reached.

# Valid *parameter* names include:
#
# * 'path'           : Base directory to store column and object vectors.
# * 'storage'        : The storage volume corresponding to the persist tier,
#                      for managed storage volumes. Must be the 'vol<#>' for a
#                      configured storage volume. Do not specify a 'default' as each
#                      rank and graph server should have their own.
#                      storage volumes (unlisted, as there is no default value).
# * 'limit'          : The maximum disk usage (bytes) per rank for this tier
#                      across all resource groups.  Default is "-1", signifying
#                      no limit and ignore watermark settings.
# * 'high_watermark' : Disk percentage used eviction threshold.  Once disk
#                      usage exceeds this value, evictions from this tier to cold
#                      storage (if configured) will be scheduled in the
#                      background and continue until the 'low_watermark'
#                      percentage usage is reached.  Default is "90", signifying
#                      a 90% disk usage threshold.
# * 'low_watermark'  : Disk percentage used recovery threshold.  Once disk usage
#                      exceeds the 'high_watermark', evictions will continue
#                      until disk usage falls below this recovery threshold.
#                      Default is "80", signifying a 80% disk usage threshold.
#
# NOTE: 'path' and 'storage' are the only applicable parameters for 'text' and 'graph'
# 
# Example showing a rank0 configuration:
#
## tier.persist.rank0.path = /opt/data_rank0
## tier.persist.rank0.storage = vol0
## tier.persist.rank0.limit = -1
## tier.persist.rank0.high_watermark = 90
## tier.persist.rank0.low_watermark = 80

tier.persist.default.path = ${gaia.persist_directory}
tier.persist.default.limit = -1
tier.persist.default.high_watermark = 90
tier.persist.default.low_watermark = 80


# ------------------------------------------------------------------------------
# Cold Storage Tier
#
# Cold Storage Tiers can be used to extend the storage capacity of the Persist Tier.
# Assign a tier strategy with cold storage to objects that will be infrequently
# accessed since they will be moved as needed from the Persist Tier.
# The Cold Storage Tier is typically a much larger capacity physical disk or
# a cloud-based storage system which may not be as performant as the Persist
# Tier storage.
#
# A default storage limit and eviction thresholds can be set across all ranks
# for a given Cold Storage Tier, while one or more ranks within a Cold Storage
# Tier may be configured to override those defaults.
#
# NOTE: If an object needs to be pulled out of cold storage during a query,
# it may need to use the local persist directory as a temporary swap space.
# This may trigger an eviction of other persisted items to cold storage due to
# low disk space condition defined by the watermark settings for the Persist Tier.
#
# The general format for Cold Storage settings:
#
## tier.cold<#>.[default|rank<#>].<parameter>
#
# Multiple Cold Storage Tiers may be defined such as 'cold', 'cold0', 'cold1',
# ... etc. to support different tiering strategies that use any one of the Cold
# Storage Tiers. A tier strategy can have, at most, one Cold Storage Tier.
# Create multiple tier strategies to use more than one Cold Storage Tier, one
# per strategy.  See 'tier_strategy' parameter for usage.

# Valid *parameter* names include:
#
# * 'type'                   : The storage provider type. Currently supports "disk"
#                              (local/network storage), "hdfs" (Hadoop distributed filesystem),
#                              "azure" (Azure blob storage), "s3" (Amazon S3 bucket) and "gcs"
#                              (Google Cloud Storage bucket).
# * 'base_path'              : A base path based on the provider type for this tier.
# * 'wait_timeout'           : Timeout in seconds for reading from or writing to this
#                              storage provider. This value should ideally be less than the value
#                              for 'tier.global.concurrent_wait_timeout' to allow concurrent queries
#                              sufficient time to acquire this resource during normal tiering
#                              operations with some slack to accomodate the request.
# * 'connection_timeout'     : Timeout in seconds for connecting to this storage provider.
# * 'use_managed_credentials': If true, use cloud provider user settings from the environment.
#                              If false, and no credentials are supplied, use anonymous access.
#                              This option applies only to "azure", "gcs", and "s3" providers.
# * 'use_https'              : This optional field can be used to override the default scheme for
#                              the storage endpoint where applicable.
#                              If true, use the 'https' scheme (default), otherwise use 'http'.
#
# HDFS-specific *parameter* names:
#
# * 'hdfs_uri'               : The host IP address & port for the hadoop distributed file system.
#                              For example:  hdfs://localhost:8020
# * 'hdfs_principal'         : The effective principal name to use when connecting to the hadoop
#                              cluster.
# * 'hdfs_use_kerberos'      : Set to "true" to enable Kerberos authentication to an HDFS storage
#                              server. The credentials of the principal are in the file specified
#                              by the 'hdfs_kerberos_keytab' parameter. Note that Kerberos's *kinit*
#                              command will be run when the database is started.
# * 'hdfs_kerberos_keytab'   : The Kerberos keytab file used to authenticate the "gpudb" Kerberos
#                              principal.
#
# Amazon S3-specific *parameter* names:
#
# * 's3_bucket_name'
# * 's3_region'                  (optional)
# * 's3_endpoint'                (optional)
# * 's3_aws_access_key_id'       (optional)
# * 's3_aws_secret_access_key'   (optional)
# * 's3_aws_role_arn'            (optional)
# * 's3_encryption_type'     :   This is optional and valid values are sse-s3 (Encryption key is managed by Amazon S3)
#                                and sse-kms (Encryption key is managed by AWS Key Management Service (kms)).
# * 's3_kms_key_id'          :   This is optional and must be specified when encryption type is sse-kms.
# * 's3_encryption_customer_algorithm' :   This is optional and must be specified when encryption type is sse-c.
# * 's3_encryption_customer_key' :   This is optional and must be specified when encryption type is sse-c.
# * 's3_use_virtual_addressing'  :   If true (default), S3 endpoints will be constructed using the 'virtual' style which
#                                    includes the bucket name as part of the hostname. Set to false to use the 'path'
#                                    style which treats the bucket name as if it is a path in the URI.
# * 's3_verify_ssl'              :   Set to false for testing purposes or when it's necessary to get pass TLS errors.
#                                    (e.g. self-signed certificates). This value is true by default.
#
# NOTE: If 's3_aws_access_key_id' and/or 's3_aws_secret_access_key' values
# are not specified, they may instead be provided by the AWS CLI or via the
# respective 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' environment
# variables.
#
# Microsoft Azure-specific *parameter* names:
#
# * 'azure_container_name'
# * 'azure_storage_account_name'
# * 'azure_endpoint'                    (optional) Specifies access to an Azure Private Link service.
# * 'azure_storage_account_key'         (optional) An Azure 'Access key' linked to a Storage account.
# * 'azure_sas_token'                   (optional) A Shared Access Signature token.
#
# Google Cloud Storage-specific *parameter* names:
#
# * 'gcs_bucket_name'
# * 'gcs_project_id'                    (optional)
# * 'gcs_service_account_id'            (optional)
# * 'gcs_service_account_private_key'   (optional)
# * 'gcs_service_account_keys'          (optional)
#
# NOTE: If the 'gcs_service_account_id', 'gcs_service_account_private_key' and/or
# 'gcs_service_account_keys' values are not specified, the Google Clould Client
# Libraries will attempt to find and use service account credentials from
# the GOOGLE_APPLICATION_CREDENTIALS environment variable.

#tier.cold0.default.type = disk
#tier.cold0.default.base_path = /mnt/gpudb/cold_storage
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 1

#tier.cold0.default.type = hdfs
#tier.cold0.default.base_path = /gpudb/cold_storage
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 30
#tier.cold0.default.hdfs_uri = hdfs://localhost:8020
#tier.cold0.default.hdfs_principal = gpudb
#tier.cold0.default.hdfs_use_kerberos = false
#tier.cold0.default.hdfs_kerberos_keytab =

#tier.cold0.default.type = s3
#tier.cold0.default.base_path = /gpudb/cold_storage
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 30
#tier.cold0.default.s3_bucket_name = my-kinetica-bucket-name
#tier.cold0.default.s3_aws_access_key_id = my-aws-access-key
#tier.cold0.default.s3_aws_secret_access_key = my-aws-secret-key
#tier.cold0.default.s3_aws_role_arn = my-aws-role-arn
#tier.cold0.default.s3_encryption_type = my-s3-sse-type
#tier.cold0.default.s3_kms_key_id = my-kms-key-id
#tier.cold0.default.s3_encryption_customer_algorithm = my-encryption-algorithm
#tier.cold0.default.s3_encryption_customer_key = my-encryption-key
#tier.cold0.default.use_managed_credentials = false

#Example with simple credential access
#tier.cold0.default.type = azure
#tier.cold0.default.base_path = /gpudb/cold_storage
#tier.cold0.default.azure_storage_account_key = my-access-key
#tier.cold0.default.azure_container_name = my-container-name
#tier.cold0.default.azure_storage_account_name = my-storage-name
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 30

#tier.cold0.default.type = azure
#tier.cold0.default.base_path = /gpudb/cold_storage
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 30
#tier.cold0.default.azure_container_name = my-kinetica-container-name
#tier.cold0.default.azure_storage_account_name = my-azure-storage-account-name
#tier.cold0.default.azure_storage_account_key = my-azure-storage-account-key
#tier.cold0.default.azure_sas_token = my-azure-sas-token
#tier.cold0.default.azure_client_id = my-azure-client-id
#tier.cold0.default.azure_client_secret = my-azure-client-secret
#tier.cold0.default.azure_tenant_id = my-azure-tenant-id
#tier.cold0.default.use_managed_credentials = false

#tier.cold0.default.type = gcs
#tier.cold0.default.base_path = /gpudb/cold_storage
#tier.cold0.default.wait_timeout = 90
#tier.cold0.default.connection_timeout = 30
#tier.cold0.default.gcs_project_id = my-kinetica-bucket-name
#tier.cold0.default.gcs_bucket_name = my-kinetica-bucket-name
#tier.cold0.default.gcs_service_account_id = my-gcs-service-account-id
#tier.cold0.default.gcs_service_account_private_key = my-gcs-service-account-private-key
#tier.cold0.default.gcs_service_account_keys = my-gcs-service-account-keys
#tier.cold0.default.use_managed_credentials = false


# ------------------------------------------------------------------------------
# Tier Strategy

# Default strategy to apply to tables or columns when one was not provided
# during table creation. This strategy is also applied to a resource group
# that does not specify one at time of creation.
#
# The strategy is formed by chaining together the tier types and their
# respective eviction priorities. Any given tier may appear no more than once
# in the chain and the priority must be in range "1" - "10", where "1" is the
# lowest priority (first to be evicted) and "9" is the highest priority (last
# to be evicted).  A priority of "10" indicates that an object is unevictable.
#
# Each tier's priority is in relation to the priority of other objects in the
# same tier; e.g., "RAM 9, DISK2 1" indicates that an object will be the
# highest evictable priority among objects in the RAM Tier (last evicted), but
# that it will be the lowest priority among objects in the Disk Tier named
# 'disk2' (first evicted).  Note that since an object can only have one Disk
# Tier instance in its strategy, the corresponding priority will only apply in
# relation to other objects in Disk Tier instance 'disk2'.
#
# See the Tiered Storage section for more information about tier type names.
#
# Format:
#
## <tier1> <priority>, <tier2> <priority>, <tier3> <priority>, ...
#
# Examples using a Disk Tier named 'disk2' and a Cold Storage Tier 'cold0':
#
## vram 3, ram 5, disk2 3, persist 10
## vram 3, ram 5, disk2 3, persist 6, cold0 10
#
tier_strategy.default = VRAM 1, RAM 5, DISK0 5, PERSIST 5

# Predicate evaluation interval (in minutes) -  indicates the interval at which
# the tier strategy predicates are evaluated
tier_strategy.predicate_evaluation_interval = 60

# ------------------------------------------------------------------------------
# Default Resource Group
#
# Resource groups are used to enforce simultaneous memory, disk and thread usage
# limits for all users within a given group. Users not assigned to a specific
# resource group will be placed within this default group. Tier-based limits are
# applied on top of existing rank tier limits.

# The scheduling priority for this group's operations, "1" - "100", where "1" is
# the lowest priority and "100" is the highest
resource_group.default.schedule_priority = 50

# The maximum eviction priority for tiered objects, "1" - "10". This supercedes
# any priorities that are set by any user provided tiering strategies.
resource_group.default.max_tier_priority = 10

# Maximum number of concurrent data operations; minimum is "4"; "-1" for no limit
resource_group.default.max_cpu_concurrency = -1

# The maximum memory (bytes) this group can use at any given time in the VRAM
# tier; "-1" for no limit
resource_group.default.vram_limit = -1

# The maximum memory (bytes) this group can use at any given time in the RAM
# tier; "-1" for no limit
resource_group.default.ram_limit = -1

# The maximum memory (bytes) this group can cumulatively use in the RAM
# tier; "-1" for no limit
resource_group.default.data_limit = -1

# ==============================================================================
# Storage Volumes
#
# When persisted rank data is stored on attached external storage volumes, the
# following config entries are used to automatically attach and mount the volume
# upon migration of a rank to another host. Attaching and mounting is performed
# by the script specified by np1.storage_api_script.
#
# The general format for storage volumes:
#
## storage.volumes.vol<#>.<parameter>
#
# Volumes are numbered starting at zero and match the number of ranks plus
# additional volumes to match graph servers.
#
# Valid *parameter* names include:
#
# * 'fs_uuid'               : The UUID identifier for the volume on the cloud
#                             provider.
# * 'id'                    : The cloud identifier of the volume.
# * 'mount_point'           : The local path to mount the cloud volume.
#                             The folder must exist and be owned by gpudb.
#
# Example:
#
## storage.volumes.vol0.fs_uuid = my_vol_uuid
## storage.volumes.vol0.id = /subscriptions/my_az_subscription_uuid/resourceGroups/my_az_rg/providers/Microsoft.Compute/
## storage.volumes.vol0.mount_point = /opt/data_rank0


# ==============================================================================
# KiFS
#
# Settings for KiFS, Kinetica's global file storage.
# There are two ways to configure KiFS:
#
# 1. (Default) Leave the settings empty. This runs KiFS in the Persist Tier.
# 2. Configure cold storage to host KiFS.
# 
# For option 2, the settings are identical to those of a cold tier as previously
# described in the Cold Storage Tier section, except prefixed by "kifs" instead 
# of 'tier.cold<#>.rank<#>'. All cold tier types are supported.
# If the 'disk' type is used, the base path must be mounted on all hosts.
# 
# Example configuration for disk:
# 
## kifs.type = disk
## kifs.base_path = /opt/gpudb/kifs
## kifs.wait_timeout = 10
## kifs.connection_timeout = 30
#
# Example configuration for AWS S3:
#
## kifs.type = s3
## kifs.base_path = gpudb/
## kifs.wait_timeout = 10
## kifs.connection_timeout = 30
## kifs.s3_bucket_name = my-kinetica-bucket-name
## kifs.s3_aws_access_key_id = my-aws-access-key
## kifs.s3_aws_secret_access_key = my-aws-secret-key
## kifs.s3_aws_role_arn = my-aws-role-arn
## kifs.s3_encryption_type = my-s3-encryption-type
## kifs.s3_kms_key_id = my-kms-key-id
## kifs.s3_encryption_customer_algorithm = my-encryption-algorithm
## kifs.s3_encryption_customer_key = my-encryption-key
## kifs.use_managed_credentials = false
#
# See Cold Storage Tier section for examples of Azure, Google Cloud Storage and HDFS settings


# The default maximum capacity of a KiFS directory (bytes); "-1" for no limit
kifs.options.directory_data_limit = 4294967296

